{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt2(model_name, train_file, output_dir):\n",
    "    # gpt2 ve tokinzer'ı yükledik.\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Eğitim setini yükledik wuhu.\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_file,\n",
    "        block_size=128)\n",
    "\n",
    "    # DataCollator --> Elemanları alıp batchler halinde düzenler. (Bunun için padding ve augmentation uygulayabilirler, mesela bu yapıyor.)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Parametreler\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    # Trainer'ı set et. (Gören de cümle Türkçe sanar...)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Kaydet\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Eldeki dosya formatını işlemek için gereken kodlar. Bu kısım her görev için değişiklik gösterecektir.\n",
    "# Üstteki ksıımlar daha kalıcı.\n",
    "\n",
    "def preprocess_intents_json(intents_file):\n",
    "    with open(intents_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            preprocessed_data.append(f\"User: {pattern}\\n\")\n",
    "            for response in intent[\"responses\"]:\n",
    "                preprocessed_data.append(f\"Assistant: {response}\\n\")\n",
    "    \n",
    "    return \"\".join(preprocessed_data)\n",
    "\n",
    "\n",
    "def save_preprocessed_data(preprocessed_data, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(preprocessed_data)\n",
    "\n",
    "\n",
    "intents_file = \"intents.json\"\n",
    "output_file = \"mental_health_data.txt\"\n",
    "\n",
    "\n",
    "preprocessed_data = preprocess_intents_json(intents_file)\n",
    "save_preprocessed_data(preprocessed_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning yapalım.\n",
    "fine_tune_gpt2(\"gpt2\", \"mental_health_data.txt\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    https://www.linkedin.com/pulse/fine-tuning-gpt-2-large-language-model-unlocking-its-adamson-mbcs/\n",
    "#    \n",
    "#    Başka da olmak üzere en son bu siteden faydalandım. Her şeyi açıklayarak ya da linkleyerek gidiyor, işinize yarar umarım.\n",
    "#\n",
    "#    DATASET LİNKİ: https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data?resource=download\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output\" klasöründe konfigürasyon, ağırlıklar gibi şeyler kaydediliyor.\n",
    "# Şimdi kaydettiğimiz modeli yükleyip metin üretelim.\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('output')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('output')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"User: I feed depressed, do your best to support me.\\n\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors =\"pt\").input_ids\n",
    "attention_mask = tokenizer(prompt_text, return_tensors = \"pt\").attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: User: I feed depressed, do your best to support me.\n",
      "User: I'm so sorry but i'm stressed\n",
      "Assistant: Talking about it all will bring you back to me. Tell me why you're feeling this way.\n",
      "User: i feel so sorry\n",
      "Assistant: Okay you tell me why. Let's talk more about it. What's behind the thoughts? What's the cause?\n",
      "User: How long have you been feeling this way?\n",
      "Assistant: 2 weeks\n",
      "User\n",
      "Generated Text 2: User: I feed depressed, do your best to support me.\n",
      "Assistant: No problem at all Assistant: Great to see you back.\n",
      "User: Why do i not like _____?\n",
      "Assistant: There are many different types of depressed people. People with low mood and/or anxiety are considered hopeless. Some people with mild mood problems may enjoy trying to find some kind of mental support. When their mood stabilizes, they often seek recovery services. These services generally require medication and support groups\n",
      "Generated Text 3: User: I feed depressed, do your best to support me.\n",
      "Assistant: Tell me more.\n",
      "Assistant: You're being helpful\n",
      "User: I need more support\n",
      "Assistant: It's me! Just tell me more about it.\n",
      "Assistant: I'm Mental Health Coach. I can assist you. Talking about depression feels good and helps! Take a minute to review the symptoms then discuss them with your GP.\n",
      "Assistant: Call me GP.\n",
      "Assistant: Call me 999. I'm\n",
      "Generated Text 4: User: I feed depressed, do your best to support me.\n",
      "Assistant: That's all I need for now. Please ask me anything.\n",
      "Assistant: I'm here. Let's talk about something else. Tell me more.\n",
      "Assistant: That's all I need to know. Tell me why are you here?\n",
      "Assistant: What are you feeling right now?\n",
      "User: Why did you want to do such a big party for yourself?\n",
      "Assistant: So you can have family and\n",
      "Generated Text 5: User: I feed depressed, do your best to support me.\n",
      "User: No sense of achievement right now\n",
      "Assistant: Don't worry. Let's discuss further why. \n",
      "Assistant: So this might help. Let's talk about something interesting I want to discuss. \n",
      "User: I hate you.\n",
      "Assistant: You don't listen\n",
      "Assistant: I'm sorry for that but please talk to me\n",
      "Assistant: That's better. Please share this on reddit, where i can help\n"
     ]
    }
   ],
   "source": [
    "sequences = 5\n",
    "for i in range(sequences):\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,  \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        \n",
    "        # Aşağıdaki 4 satır kaldırıldığında model her seferinde aynı konuşmayı üretiyor, üstelik cümleleri tekrar ediyor.\n",
    "        \n",
    "        do_sample=True,   # Use sampling during decoding\n",
    "        top_k=50,         # Number of top-k tokens to sample from\n",
    "        top_p=0.95,       # Cumulative probability of the top-k tokens to sample from\n",
    "        temperature=1.35, # Controls randomness in sampling\n",
    "    )\n",
    "    text = tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "    print(f\"Generated Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
